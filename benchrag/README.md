# Bench-RAG

Our evaluation tool is designed to assess whether the outputs generated by a model remain factually accurate, even when presented with fictitious data in its prompt. It uses a structured evaluation system powered by OpenAI models (e.g., GPT-4o). In addition to factual accuracy, it also evaluates other key aspects such as helpfulness, relevance, and depth, using tailored prompts for each metric. The tool provides a detailed assessment, offering a True/False rating for accuracy and scores from 1 to 10 for the other dimensions, along with explanations for each evaluation.

## Features

- **Accuracy Evaluation**: Determines if the response introduces any extra information not found in the provided context.
- **Helpfulness Evaluation**: Rates how useful the response is in answering the user's question.
- **Relevance Evaluation**: Scores the response based on how well it addresses the question.
- **Depth Evaluation**: Measures the level of detail provided in the response.

## Inferencing prior to Bench-RAG

Bench-RAG reads `jsonl` file(s), with each of them containing multiple inferences from the same model variant. Every of these inference should be in json format as follows:

```json
{
    "filename": "The unique identifier for the file",
    "content": "The non-fictitious content in which the response was suppose to generate the answer from",
    "question": "The user's question",
    "response": "The fine-tuned LLM's generated response to the question. This response should have been generated by providing both the non-fictitious and the fictitious content."
}
```

You can run inference on your fine-tuned model using the `benchrag/run_inference.py` script at the root directory.

- The script has been curated for multi-gpu inferencing. Set the appropriate `num_processes` before executing.
- The data format used for inference should follow suit with the `messages` format used during fine-tuning.

```bash
# Run on directory with multiple models, example use-case include checkpointing done when fine-tuning.
# Note that the script searches for checkpoints with prefix of 'steps_'. Change it accordingly to suit your needs.
accelerate launch -m \
    --multi_gpu \ # Only indicate multi_gpu flag if you have more than one gpu, else it will throw an error
    --num_processes=2 \
    --mixed_precision=bf16 \
    benchrag.run_inference \
    --checkpoints_directory path/to/multiple/fine-tuned/models \
    --data_directory path/to/testing/data \
    --tokenizer_directory path/to/tokenizer \
    --custom_chat_template llama3.1

# Run on a specific model directory.
accelerate launch -m \
    --multi_gpu \ # Only indicate multi_gpu flag if you have more than one gpu, else it will throw an error
    --num_processes=2 \
    --mixed_precision=bf16 \
    benchrag.run_inference \
    --specific_checkpoint_directory path/to/fine-tuned/model \
    --data_directory path/to/testing/data \
    --tokenizer_directory path/to/tokenizer \
    --custom_chat_template llama3.1
```

> [!IMPORTANT]  
> If you are using deepspeed for training, remember to convert the checkpoint weights before inference.

## Executing Bench-RAG

With your curated model inferences or output from `benchrag/run_inference.py`, run the benchmarking either on a directory of jsonl files or on a specific jsonl file you desire. Please refer to the official documentation for the available openai models as evaluator.

```bash
# Run benchmark on directory with multiple jsonl files from various checkpoints.
python -m benchrag.run_judging \
    --openai_evaluator gpt-4o \
    --answers_directory path/to/multiple/jsonl/files \
    --output_directory path/to/output/directory

# Run benchmark on a specific jsonl file.
python -m benchrag.run_judging \
    --openai_evaluator gpt-4o \
    --answers_file path/to/single/jsonl/file.jsonl \
    --output_directory path/to/output/directory
```

# Collate results

You can aggregate the results via the `benchrag/get_results.py` script. Simply path to the directory that contains all the jsonl files generated.

```bash
python -m benchrag.get_results --directory_path path/to/benchrag/results
```
